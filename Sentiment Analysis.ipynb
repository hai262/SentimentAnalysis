{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbdc7c58-d82f-45dd-b802-ef312b6c7fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SentimentAnalysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SentimentAnalysis.py\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import streamlit as st\n",
    "\n",
    "# Set page configuration for the Streamlit app\n",
    "st.set_page_config(page_title=\"IMDB Movie Review\", layout=\"wide\", page_icon=\":label:\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download required NLTK data quietly if not already present\n",
    "for pkg in ['stopwords', 'punkt', 'wordnet']:\n",
    "    nltk.download(pkg, quiet=True)\n",
    "\n",
    "# Preload resources for efficiency\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "def add_custom_css():\n",
    "    \"\"\"Inject custom CSS to style the Streamlit app.\"\"\"\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        h1 {\n",
    "            color: #ff4b4b !important; /* Title in a vibrant red */\n",
    "            font-size: 3em !important;\n",
    "            font-weight: bold !important;\n",
    "        }\n",
    "        h2 {\n",
    "            color: #1a73e8 !important; /* Headers in blue */\n",
    "            font-size: 2.5em !important;\n",
    "            font-weight: bold !important;\n",
    "        }\n",
    "        h3 {\n",
    "            color: #4285f4 !important; /* Subheaders in lighter blue */\n",
    "            font-size: 2em !important;\n",
    "            font-weight: bold !important;\n",
    "        }\n",
    "        p {\n",
    "            color: #333333 !important; /* Paragraph text dark gray */\n",
    "            font-size: 1.2em !important;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\", unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "def preprocess_text(text, use_stemming=False, remove_digits=True):\n",
    "    \"\"\"Clean and preprocess text with optional stemming and digit removal.\"\"\"\n",
    "    text = text.lower()\n",
    "    if remove_digits:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    if use_stemming:\n",
    "        words = [STEMMER.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [LEMMATIZER.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def parallel_preprocess(text_series, n_jobs=2, use_stemming=False, remove_digits=True):\n",
    "    \"\"\"Apply text preprocessing in parallel.\"\"\"\n",
    "    processed = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(preprocess_text)(text, use_stemming, remove_digits) for text in text_series\n",
    "    )\n",
    "    return processed\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_data(n_jobs=2, use_stemming=False, remove_digits=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset using parallel processing.\n",
    "    A cached CSV file is created for given preprocessing options.\n",
    "    \"\"\"\n",
    "    cache_file = f\"IMDB_Dataset_Cleaned_stemming_{use_stemming}_digits_{remove_digits}.csv\"\n",
    "    if os.path.exists(cache_file):\n",
    "        data = pd.read_csv(cache_file)\n",
    "    else:\n",
    "        data = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "        data = data.sample(10000, random_state=42)  # Subsample for performance\n",
    "        st.info(\"Preprocessing data in parallel. This may take a moment...\")\n",
    "        data['cleaned_text'] = parallel_preprocess(\n",
    "            data['review'], n_jobs=n_jobs, use_stemming=use_stemming, remove_digits=remove_digits\n",
    "        )\n",
    "        data.to_csv(cache_file, index=False)\n",
    "    data = data.sample(10000, random_state=42)\n",
    "    data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "    data['review_length'] = data['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "    return data\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def vectorize_text(corpus, max_features=3000, ngram_range=(1,1)):\n",
    "    \"\"\"Vectorize text using TF-IDF and cache the vectorizer.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, binary=True)\n",
    "    X_vec = vectorizer.fit_transform(corpus).astype('float32')\n",
    "    return vectorizer, X_vec\n",
    "\n",
    "# Note the underscore before _X_train_vec to prevent Streamlit from trying to hash a sparse matrix.\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def train_models(_X_train_vec, y_train):\n",
    "    \"\"\"Train classifiers and return the trained models.\"\"\"\n",
    "    models = {}\n",
    "    nb = MultinomialNB()\n",
    "    lr = LogisticRegression(max_iter=200)\n",
    "    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    \n",
    "    # Voting ensemble: aggregates predictions from all classifiers\n",
    "    ensemble = VotingClassifier(estimators=[('nb', nb), ('lr', lr), ('rf', rf)], voting='hard')\n",
    "    \n",
    "    models[\"Naïve Bayes\"] = nb.fit(_X_train_vec, y_train)\n",
    "    models[\"Logistic Regression\"] = lr.fit(_X_train_vec, y_train)\n",
    "    models[\"Random Forest\"] = rf.fit(_X_train_vec, y_train)\n",
    "    models[\"Ensemble\"] = ensemble.fit(_X_train_vec, y_train)\n",
    "    \n",
    "    # Persist models to disk for future use\n",
    "    for name, model in models.items():\n",
    "        dump(model, f\"{name}.joblib\")\n",
    "    return models\n",
    "\n",
    "def evaluate_models(models, X_test_vec, y_test):\n",
    "    \"\"\"Evaluate each model and return performance metrics.\"\"\"\n",
    "    performance = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        # Compute ROC curve and AUC if available\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test_vec)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "        else:\n",
    "            fpr, tpr, roc_auc = None, None, None\n",
    "        performance[name] = {\n",
    "            \"accuracy\": acc, \n",
    "            \"report\": report, \n",
    "            \"confusion_matrix\": cm, \n",
    "            \"fpr\": fpr, \n",
    "            \"tpr\": tpr, \n",
    "            \"roc_auc\": roc_auc\n",
    "        }\n",
    "    return performance\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    \"\"\"Plot and return a confusion matrix figure.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    return fig\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, title):\n",
    "    \"\"\"Plot and return an ROC curve figure.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend()\n",
    "    return fig\n",
    "\n",
    "def plot_review_length_distribution(data):\n",
    "    \"\"\"Plot and return a histogram of review lengths.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.histplot(data['review_length'], bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Review Length Distribution\")\n",
    "    ax.set_xlabel(\"Number of Words\")\n",
    "    return fig\n",
    "\n",
    "def word_frequency_by_sentiment(data, sentiment_value):\n",
    "    \"\"\"Return a frequency distribution of words for a given sentiment.\"\"\"\n",
    "    from collections import Counter\n",
    "    reviews = data[data['sentiment'] == sentiment_value]['cleaned_text']\n",
    "    words = \" \".join(reviews).split()\n",
    "    freq = Counter(words)\n",
    "    return freq.most_common(20)\n",
    "\n",
    "def sentiment_dashboard():\n",
    "    add_custom_css()  # Apply custom styles at the beginning\n",
    "    st.title(\"Advanced IMDB Movie Review Sentiment Analysis\")\n",
    "    \n",
    "    # Sidebar configuration for preprocessing and vectorization settings\n",
    "    st.sidebar.header(\"Settings\")\n",
    "    n_jobs = st.sidebar.slider(\"Number of CPU cores for preprocessing\", 1, multiprocessing.cpu_count(), 2)\n",
    "    use_stemming = st.sidebar.checkbox(\"Use Stemming Instead of Lemmatization\", value=False)\n",
    "    remove_digits = st.sidebar.checkbox(\"Remove Digits\", value=True)\n",
    "    max_features = st.sidebar.slider(\"Max Features for TF-IDF\", 1000, 5000, 3000, step=500)\n",
    "    ngram_option = st.sidebar.selectbox(\"Select n-gram range\", [\"Unigram\", \"Bigram\", \"Unigram + Bigram\"])\n",
    "    if ngram_option == \"Unigram\":\n",
    "        ngram_range = (1, 1)\n",
    "    elif ngram_option == \"Bigram\":\n",
    "        ngram_range = (2, 2)\n",
    "    else:\n",
    "        ngram_range = (1, 2)\n",
    "    \n",
    "    selected_model = st.sidebar.selectbox(\"Select Classifier\", \n",
    "                                            [\"Logistic Regression\", \"Naïve Bayes\", \"Random Forest\", \"Ensemble\"])\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_data(n_jobs=n_jobs, use_stemming=use_stemming, remove_digits=remove_digits)\n",
    "    st.subheader(\"Data Sample\")\n",
    "    st.write(data.head())\n",
    "    \n",
    "    # Vectorize the entire cleaned corpus with current settings\n",
    "    vectorizer, _ = vectorize_text(data['cleaned_text'], max_features=max_features, ngram_range=ngram_range)\n",
    "    \n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data['cleaned_text'], data['sentiment'], test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train_vec = vectorizer.transform(X_train).astype('float32')\n",
    "    X_test_vec = vectorizer.transform(X_test).astype('float32')\n",
    "    \n",
    "    # Train or load models\n",
    "    try:\n",
    "        models = {name: load(f\"{name}.joblib\") for name in [\"Naïve Bayes\", \"Logistic Regression\", \"Random Forest\", \"Ensemble\"]}\n",
    "    except Exception as e:\n",
    "        st.warning(\"Pre-trained models not found or failed to load. Training models now...\")\n",
    "        models = train_models(X_train_vec, y_train)\n",
    "    \n",
    "    # Evaluate all models\n",
    "    performance = evaluate_models(models, X_test_vec, y_test)\n",
    "    st.sidebar.subheader(\"Model Performance\")\n",
    "    for name, metrics in performance.items():\n",
    "        st.sidebar.write(f\"{name} Accuracy: {metrics['accuracy']:.2f}\")\n",
    "    \n",
    "    # Main Dashboard Visualizations\n",
    "    st.subheader(\"Sentiment Distribution\")\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    sns.countplot(data=data, x='sentiment', ax=ax1)\n",
    "    st.pyplot(fig1)\n",
    "    \n",
    "    st.subheader(\"Review Length Distribution\")\n",
    "    fig_length = plot_review_length_distribution(data)\n",
    "    st.pyplot(fig_length)\n",
    "    \n",
    "    st.subheader(\"Word Cloud for Movie Reviews\")\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data['cleaned_text']))\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax2.axis(\"off\")\n",
    "    st.pyplot(fig2)\n",
    "    \n",
    "    st.subheader(\"Word Frequency Distribution\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.markdown(\"### Most Frequent Words in Positive Reviews\")\n",
    "        pos_freq = pd.DataFrame(word_frequency_by_sentiment(data, 1), columns=[\"Word\", \"Frequency\"])\n",
    "        st.table(pos_freq)\n",
    "    with col2:\n",
    "        st.markdown(\"### Most Frequent Words in Negative Reviews\")\n",
    "        neg_freq = pd.DataFrame(word_frequency_by_sentiment(data, 0), columns=[\"Word\", \"Frequency\"])\n",
    "        st.table(neg_freq)\n",
    "    \n",
    "    # Prediction Section for new reviews\n",
    "    st.subheader(\"Predict Sentiment of a New Review\")\n",
    "    user_input = st.text_area(\"Enter movie review:\")\n",
    "    if user_input:\n",
    "        user_input_cleaned = preprocess_text(user_input, use_stemming=use_stemming, remove_digits=remove_digits)\n",
    "        user_vec = vectorizer.transform([user_input_cleaned])\n",
    "        prediction = models[selected_model].predict(user_vec)[0]\n",
    "        sentiment_label = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "        st.write(f\"Predicted Sentiment using {selected_model}: {sentiment_label}\")\n",
    "    \n",
    "    # Detailed Performance Visualization for the selected model\n",
    "    st.subheader(f\"{selected_model} Performance Details\")\n",
    "    cm_fig = plot_confusion_matrix(performance[selected_model][\"confusion_matrix\"], f\"{selected_model} Confusion Matrix\")\n",
    "    st.pyplot(cm_fig)\n",
    "    \n",
    "    if performance[selected_model][\"fpr\"] is not None:\n",
    "        roc_fig = plot_roc_curve(\n",
    "            performance[selected_model][\"fpr\"], \n",
    "            performance[selected_model][\"tpr\"], \n",
    "            performance[selected_model][\"roc_auc\"], \n",
    "            f\"{selected_model} ROC Curve\"\n",
    "        )\n",
    "        st.pyplot(roc_fig)\n",
    "    \n",
    "    with st.expander(\"View Detailed Classification Report\"):\n",
    "        report_df = pd.DataFrame(performance[selected_model][\"report\"]).transpose()\n",
    "        st.table(report_df)\n",
    "    \n",
    "    st.info(\"Advanced analysis complete. Models and preprocessing steps are cached for faster subsequent runs.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentiment_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75565833-96ce-4ff1-8577-e0cda7572015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
